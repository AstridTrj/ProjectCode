{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# model\n",
    "# author = 'huangth'\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "BASE_PATH = 'D:\\\\Azy\\\\'\n",
    "tt=pd.read_csv(\"D:\\\\Azy\\\\RawData\\\\train_dataset.csv\")\n",
    "ETL_DATA_PATH = os.path.join(BASE_PATH, \"EtlData\")\n",
    "\n",
    "\n",
    "def get_feature(name):\n",
    "    data_name = os.path.join(ETL_DATA_PATH, \"{}.csv\".format(name))\n",
    "    df = pd.read_csv(data_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def lgb_mae_model(train_df, test_df, params):\n",
    "    NFOLDS = 10\n",
    "    train_label = train_df['信用分']\n",
    "    kfold = KFold(n_splits=NFOLDS, shuffle=False, random_state=2019)\n",
    "    kf = kfold.split(train_df, train_label)\n",
    "\n",
    "    train = train_df.drop(['用户编码', '信用分'], axis=1)\n",
    "    test = test_df.drop(['用户编码'], axis=1)\n",
    "\n",
    "    cv_pred = np.zeros(test.shape[0])\n",
    "    valid_best_l2_all = 0\n",
    "\n",
    "    count = 0\n",
    "    for i, (train_fold, validate) in enumerate(kf):\n",
    "        print(\"model: lgb_mae. fold: \", i , \"training...\")\n",
    "        X_train, label_train = train.iloc[train_fold], train_label.iloc[train_fold]\n",
    "        X_validate, label_validate = train.iloc[validate], train_label.iloc[validate]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_train, label_train)\n",
    "        dvalid = lgb.Dataset(X_validate, label_validate, reference=dtrain)\n",
    "\n",
    "        bst = lgb.train(params, dtrain, num_boost_round=10000, valid_sets=dvalid, verbose_eval=-1, early_stopping_rounds=50)\n",
    "        cv_pred += bst.predict(test, num_iteration=bst.best_iteration)\n",
    "        valid_best_l2_all += bst.best_score['valid_0']['l1']\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    cv_pred /= NFOLDS\n",
    "    valid_best_l2_all /= NFOLDS\n",
    "    print(\"lgb_mae cv score for valid is: \", 1/(1+valid_best_l2_all))\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"lgb_mae  feature importance：\")\n",
    "    fea_importances = pd.DataFrame({\n",
    "        'column': train.columns,\n",
    "        'importance': bst.feature_importance(importance_type='split', iteration=bst.best_iteration)\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    print(fea_importances)\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "    return cv_pred\n",
    "\n",
    "\n",
    "def lgb_mse_model(train_df, test_df, params):\n",
    "    NFOLDS = 10\n",
    "    train_label = train_df['信用分']\n",
    "    kfold = KFold(n_splits=NFOLDS, shuffle=False, random_state=2019)\n",
    "    kf = kfold.split(train_df, train_label)\n",
    "\n",
    "    train = train_df.drop(['用户编码', '信用分'], axis=1)\n",
    "    test = test_df.drop(['用户编码'], axis=1)\n",
    "\n",
    "    cv_pred = np.zeros(test.shape[0])\n",
    "    valid_best_l2_all = 0\n",
    "\n",
    "    count = 0\n",
    "    for i, (train_fold, validate) in enumerate(kf):\n",
    "        print(\"model:lgb_mse. fold: \", i , \"training...\")\n",
    "        X_train, label_train = train.iloc[train_fold], train_label.iloc[train_fold]\n",
    "        X_validate, label_validate = train.iloc[validate], train_label.iloc[validate]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_train, label_train)\n",
    "        dvalid = lgb.Dataset(X_validate, label_validate, reference=dtrain)\n",
    "\n",
    "        bst = lgb.train(params, dtrain, num_boost_round=10000, valid_sets=dvalid, verbose_eval=-1, early_stopping_rounds=50)\n",
    "        cv_pred += bst.predict(test, num_iteration=bst.best_iteration)\n",
    "        valid_best_l2_all += bst.best_score['valid_0']['l1']\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    cv_pred /= NFOLDS\n",
    "    valid_best_l2_all /= NFOLDS\n",
    "    print(\"lgb_mse cv score for valid is: \", 1/(1+valid_best_l2_all))\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"lgb_mse  feature importance：\")\n",
    "    fea_importances = pd.DataFrame({\n",
    "        'column': train.columns,\n",
    "        'importance': bst.feature_importance(importance_type='split', iteration=bst.best_iteration)\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    print(fea_importances)\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "    return cv_pred\n",
    "\n",
    "\n",
    "def xgb_mae_model(train_df, test_df, params):\n",
    "    NFOLDS = 5\n",
    "    train_label = train_df['信用分']\n",
    "    kfold = KFold(n_splits=NFOLDS, shuffle=False, random_state=2019)\n",
    "    kf = kfold.split(train_df, train_label)\n",
    "\n",
    "    train = train_df.drop(['用户编码', '信用分'], axis=1)\n",
    "    test = test_df.drop(['用户编码'], axis=1)\n",
    "\n",
    "    cv_pred = np.zeros(test.shape[0])\n",
    "\n",
    "    count = 0\n",
    "    preds_list = list()\n",
    "    oof = np.zeros(train_df.shape[0])\n",
    "    for i, (train_fold, validate) in enumerate(kf):\n",
    "        print(\"model: xgb_mae. fold: \", i , \"training...\")\n",
    "        X_train, label_train = train.iloc[train_fold], train_label.iloc[train_fold]\n",
    "        X_validate, label_validate = train.iloc[validate], train_label.iloc[validate]\n",
    "\n",
    "        gbm = xgb.XGBRegressor(**params)\n",
    "        bst = gbm.fit(X_train, label_train, eval_set=[(X_train, label_train), (X_validate, label_validate)],\n",
    "                          early_stopping_rounds=200, verbose=500)\n",
    "\n",
    "        k_pred = bst.predict(X_validate)\n",
    "        oof[validate] = k_pred\n",
    "\n",
    "        preds = gbm.predict(test)\n",
    "        preds_list.append(preds)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    fold_mae_error = mean_absolute_error(train_label, oof)\n",
    "def cat_model(train_df, test_df, params):\n",
    "    NFOLDS = 5\n",
    "    train_label = train_df['信用分']\n",
    "    kfold = KFold(n_splits=NFOLDS, shuffle=False, random_state=2019)\n",
    "    kf = kfold.split(train_df, train_label)\n",
    "\n",
    "    train = train_df.drop(['用户编码', '信用分'], axis=1)\n",
    "    test = test_df.drop(['用户编码'], axis=1)\n",
    "\n",
    "    cv_pred = np.zeros(test.shape[0])\n",
    "\n",
    "    count = 0\n",
    "    preds_list = list()\n",
    "    oof = np.zeros(train_df.shape[0])\n",
    "    for i, (train_fold, validate) in enumerate(kf):\n",
    "        print(\"model: xgb_mae. fold: \", i , \"training...\")\n",
    "        X_train, label_train = train.iloc[train_fold], train_label.iloc[train_fold]\n",
    "        X_validate, label_validate = train.iloc[validate], train_label.iloc[validate]\n",
    "\n",
    "        cat = CatBoostRegressor(**params)\n",
    "        bst = cat.fit(X_train, label_train, eval_set=[(X_train, label_train), (X_validate, label_validate)],\n",
    "                          early_stopping_rounds=1000, verbose=500)\n",
    "\n",
    "        k_pred = bst.predict(X_validate)\n",
    "        oof[validate] = k_pred\n",
    "\n",
    "        preds = cat.predict(test)\n",
    "        preds_list.append(preds)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    fold_mae_error = mean_absolute_error(train_label, oof)\n",
    "\n",
    "\n",
    "    preds_columns = ['preds_{id}'.format(id=i) for i in range(NFOLDS)]\n",
    "    preds_df = pd.DataFrame(data=preds_list)\n",
    "    preds_df = preds_df.T\n",
    "    preds_df.columns = preds_columns\n",
    "    preds_list = list(preds_df.mean(axis=1))\n",
    "    cv_pred = preds_list\n",
    "\n",
    "    print(\"xgb_mae cv score for valid is: \", 1/(1+fold_mae_error))\n",
    "\n",
    "    # print(\"----------------------------------------\")\n",
    "    # print(\"----------------------------------------\")\n",
    "    # print(\"xgb_mae  feature importance：\")\n",
    "    # fea_importances = pd.DataFrame({\n",
    "    #     'column': train.columns,\n",
    "    #     'importance': bst.feature_importance(importance_type='split', iteration=bst.best_iteration)\n",
    "    # }).sort_values(by='importance', ascending=False)\n",
    "    # print(fea_importances)\n",
    "    # print(\"----------------------------------------\")\n",
    "    # print(\"----------------------------------------\")\n",
    "\n",
    "    return cv_pred\n",
    "\n",
    "\n",
    "def model_bagging(pred1, pred2):\n",
    "    cv_pred = (pred1)\n",
    "    return cv_pred\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  2.81 Mb (75.4% reduction)\n",
      "Mem. usage decreased to  2.81 Mb (74.6% reduction)\n",
      "Gen train shape: (50000, 28), test shape: (50000, 27)\n",
      "features num:  26\n",
      "Feature engineering has finished!\n",
      "Cost 3.4923670291900635 s.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 特征工程\n",
    "# author = 'huangth'\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "BASE_PATH = 'D:\\\\Azy\\\\'\n",
    "RAW_DATA_PATH = os.path.join(BASE_PATH, \"RawData\")\n",
    "ETL_DATA_PATH = os.path.join(BASE_PATH, \"EtlData\")\n",
    "\n",
    "\n",
    "class Processing(object):\n",
    "    # 读取数据\n",
    "    @staticmethod\n",
    "    def _get_data_(name):\n",
    "        data_name = os.path.join(RAW_DATA_PATH, '{}.csv'.format(name))\n",
    "        df = pd.read_csv(data_name)\n",
    "        return df\n",
    "\n",
    "    # 改变变量类型节省内存空间\n",
    "    @staticmethod\n",
    "    def _reduce_mem_usage_(df, verbose=True):\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        start_mem = df.memory_usage().sum() / 1024**2\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)\n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "        return df\n",
    "\n",
    "    # 众数填充异常值\n",
    "    @staticmethod\n",
    "    def _mode_fill_(df, col):\n",
    "        df.loc[df[col] == 0, col] = df[col].mode()\n",
    "        return df\n",
    "\n",
    "    # 疯狂找特征呀呀呀\n",
    "    @staticmethod\n",
    "    def _feature_(df):\n",
    "        #df['缴费金额是否能覆盖当月账单'] = df['缴费用户最近一次缴费金额（元）'] - df['用户账单当月总费用（元）']\n",
    "        #df['最近一次缴费是否超过平均消费额'] = df['缴费用户最近一次缴费金额（元）'] - df['用户近6个月平均消费值（元）']\n",
    "        #df['当月账单是否超过平均消费额'] = df['用户账单当月总费用（元）'] - df['用户近6个月平均消费值（元）']\n",
    "\n",
    "        #df['是否去过高档商场'] = df['当月是否逛过福州仓山万达'] * df['当月是否到过福州山姆会员店']\n",
    "        #df['交通类应用使用次数'] = df['当月飞机类应用使用次数'] + df['当月火车类应用使用次数']\n",
    "\n",
    "        # 缴费方式\n",
    "        df['缴费方式'] = 0\n",
    "        df['缴费方式'][(df['缴费用户最近一次缴费金额（元）'] % 10 == 0) & df['缴费用户最近一次缴费金额（元）'] != 0] = 1\n",
    "\n",
    "        # 消费稳定性  话费/余额\n",
    "        #df['缴费稳定性'] = df['用户账单当月总费用（元）'] / (df['用户近6个月平均消费值（元）'] + 1)\n",
    "        #df['当月话费使用率'] = df['用户账单当月总费用（元）'] / (df['用户当月账户余额（元）'] + 1)\n",
    "        return df\n",
    "\n",
    "    # 年龄分箱\n",
    "    @staticmethod\n",
    "    def _group_age_(x):\n",
    "        if x <= 18:\n",
    "            return 1\n",
    "        elif x <= 30:\n",
    "            return 2\n",
    "        elif x <= 35:\n",
    "            return 3\n",
    "        elif x <= 45:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "\n",
    "    # 长尾数据处理\n",
    "    @staticmethod\n",
    "    def _log_feature_(df):\n",
    "        user_bill_features = ['缴费用户最近一次缴费金额（元）', '用户近6个月平均消费值（元）',\n",
    "                              '用户账单当月总费用（元）', '用户当月账户余额（元）']\n",
    "        log_features = ['当月网购类应用使用次数', '当月金融理财类应用使用总次数', '当月视频播放类应用使用次数']\n",
    "        for col in user_bill_features + log_features:\n",
    "            df[col] = df[col].map(lambda x: np.log1p(x))\n",
    "        return df\n",
    "    @staticmethod\n",
    "    def _search_(df):\n",
    "        rf=pd.DataFrame()\n",
    "        rf[\"用户编码\"]=df[\"用户编码\"]\n",
    "        rf[\"信用分\"]=df[\"信用分\"]\n",
    "        rf[\"当前消费动荡\"]=((df[\"用户近6个月平均消费值（元）\"]*6-df[\"用户账单当月总费用（元）\"])/5)-(df[\"用户账单当月总费用（元）\"])\n",
    "        #rf[\"当前消费动荡2\"]=((df[\"用户近6个月平均消费值（元）\"]*6-df[\"用户账单当月总费用（元）\"])/5)-(df[\"用户账单当月总费用（元）\"])\n",
    "        #rf[\"当前消费动荡a\"]=(df[\"用户近6个月平均消费值（元）\"]+1)/(df['用户近6个月平均消费值（元）']+1)\n",
    "        rf[\"用户近6个月平均消费值（元）\"]=(df[\"用户近6个月平均消费值（元）\"]*6-df[\"用户账单当月总费用（元）\"])/5\n",
    "         #rf[\"用户近6个月平均消费值（元）\"]=(df[\"用户近6个月平均消费值（元）\"]*6-df[\"用户账单当月总费用（元）\"])/5\n",
    "        rf[\"用户账单当月总费用（元）\"]=df[\"用户账单当月总费用（元）\"]       \n",
    "        rf[\"用户网龄（月）\"]=df[\"用户网龄（月）\"]\n",
    "        rf[\"当月通话交往圈人数 \"]=df[\"当月通话交往圈人数\"]\n",
    "        rf[\"用户年龄\"]=df[\"用户年龄\"]\n",
    "        rf['缴费用户最近一次缴费金额（元）']=df['缴费用户最近一次缴费金额（元）']\n",
    "        rf[\"当月金融理财类应用使用总次数\"]=df[\"当月金融理财类应用使用总次数\"]\n",
    "        rf['当月网购类应用使用次数']=df['当月网购类应用使用次数']\n",
    "        rf['当月视频播放类应用使用次数']=df['当月视频播放类应用使用次数']\n",
    "        rf['用户话费敏感度']=df['用户话费敏感度']\n",
    "        rf = pd.get_dummies(rf, columns=[\"用户话费敏感度\"])\n",
    "        rf['近三个月月均商场出现次数']=df['近三个月月均商场出现次数']\n",
    "        rf['当月话费使用率'] = df['用户账单当月总费用（元）'] / (df['用户当月账户余额（元）'] + 1)\n",
    "        rf[\"当月旅游资讯类应用使用次数\"]=df[\"当月旅游资讯类应用使用次数\"]\n",
    "        rf[\"交通类应用使用次数\"]=df['当月飞机类应用使用次数'] + df['当月火车类应用使用次数']\n",
    "        rf['是否去过高档商场'] = (1-df['当月是否逛过福州仓山万达'])*(1-df['当月是否到过福州山姆会员店'])\n",
    "        df[\"交通类应用使用次数\"]=rf[\"交通类应用使用次数\"]\n",
    "        rf['当月是否景点游览']=df['当月是否景点游览']\n",
    "        rf[\"当月是否体育场馆消费\"]=(1-df[\"当月是否体育场馆消费\"])*(1-df[\"当月是否看电影\"])\n",
    "        rf[\"当月是否体育场馆消费\"]=df[\"当月是否体育场馆消费\"]\n",
    "        rf[\"当月是否看电影\"]=df[\"当月是否看电影\"]\n",
    "        rf[\"是否4G不健康客户\"]=df[\"是否4G不健康客户\"]\n",
    "        rf[\"缴费用户当前是否欠费缴费\"]=df[\"缴费用户当前是否欠费缴费\"]\n",
    "        rf[\"当月物流快递类应用使用次数\"]=df[\"当月物流快递类应用使用次数\"]\n",
    "        #rf[\"是否大学生客户\"]=df[\"是否大学生客户\"]\n",
    "        #rf[\"是否黑名单客户\"]=df[\"是否黑名单客户\"]\n",
    "        #rf[\"用户实名制是否通过核实\"]=df[\"用户实名制是否通过核实\"]         \n",
    "        #rf[\"实名大\"]=(1-df[\"是否黑名单客户\"])*(1-df[\"是否大学生客户\"])\n",
    "        #rf[\"偏好\"]=0\n",
    "        #rf[\"偏好\"][(df[\"当月网购类应用使用次数\"]>df['当月金融理财类应用使用总次数'])&(df['当月网购类应用使用次数']>df['当月视频播放类应用使用次数'])]=1\n",
    "        #rf[\"偏好\"][(df[\"当月金融理财类应用使用总次数\"]>df['当月网购类应用使用次数'])&(df['当月金融理财类应用使用总次数']>df['当月视频播放类应用使用次数'])]=2\n",
    "        #rf = pd.get_dummies(rf, columns=[\"偏好\"])\n",
    "        #rf[\"缴费方式\"]= df[\"缴费方式\"]\n",
    "        #rf[\"是否经常逛商场的人\"]=df[\"是否经常逛商场的人\"]\n",
    "        #rf['次数'] = df['当月网购类应用使用次数'] +  df['当月物流快递类应用使用次数'] +  df['当月金融理财类应用使用总次数'] + df['当月视频播放类应用使用次数']+ df['当月飞机类应用使用次数'] + df['当月火车类应用使用次数'] + df['当月旅游资讯类应用使用次数']  + 1\n",
    "        #for col in ['当月金融理财类应用使用总次数','当月旅游资讯类应用使用次数','交通类应用使用次数','当月网购类应用使用次数','当月视频播放类应用使用次数']: # 这两个比较积极向上一点\n",
    "        #    rf[col + '百分比'] = df[col].values / rf['次数'].values \n",
    "        #rf[\"出行\"]=rf[\"当月旅游资讯类应用使用次数\"]+rf[\"交通类应用使用次数\"]\n",
    "        #rf[\"bili\"]=rf[\"用户网龄（月）\"]/rf[\"用户年龄\"]\n",
    "        #rf['']\n",
    "        #rf[\"比例\"]=df[\"用户近6个月平均消费值（元）\"]/df[\"用户当月账户余额（元）\"]\n",
    "        #rf[\"偏好\"]=0\n",
    "        #rf[\"偏好\"][(df[\"当月网购类应用使用次数\"]>df['当月金融理财类应用使用总次数'])&(df['当月网购类应用使用次数']>df['当月视频播放类应用使用次数'])]=1\n",
    "        #rf[\"偏好\"][(df[\"当月金融理财类应用使用总次数\"]>df['当月网购类应用使用次数'])&\n",
    "        #         (df['当月金融理财类应用使用总次数']>df['当月视频播放类应用使用次数'])]=2\n",
    "        #rf = pd.get_dummies(rf, columns=[\"偏好\"])\n",
    "        #rf[\"用户当月账户余额（元）\"]=df[\"用户当月账户余额（元）\"]\n",
    "        #rf['当月通话人均话费'] = df['用户账单当月总费用（元）'].values / (df['当月通话交往圈人数'].values + 1)\n",
    "        #rf['上个月费用'] = df['用户当月账户余额（元）'].values + df['用户账单当月总费用（元）'].values-df[\"缴费用户最近一次缴费金额（元）\"].values\n",
    "        #rf['用户上网年龄'] = df['用户年龄'] - df['用户网龄（月）']/12\n",
    "        return rf\n",
    "    @staticmethod\n",
    "    def get_recharge_way(item):\n",
    "        # 是否能被10整除\n",
    "        if item == 0:\n",
    "            return -1\n",
    "        if item % 10 == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def get_processing(self):\n",
    "        train_df = self._get_data_('train_dataset')\n",
    "        test_df = self._get_data_('test_dataset')\n",
    "\n",
    "        train_df = self._reduce_mem_usage_(train_df)\n",
    "        test_df = self._reduce_mem_usage_(test_df)\n",
    "\n",
    "        test_df['信用分'] = -1\n",
    "        data = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "\n",
    "        del train_df, test_df\n",
    "        gc.collect()\n",
    "        #data['缴费方式'] = 0\n",
    "        #data['缴费方式']=data['缴费用户最近一次缴费金额（元）'].apply(self.get_recharge_way)\n",
    "        data = self._mode_fill_(data, '用户账单当月总费用（元）')\n",
    "        data = self._mode_fill_(data, '用户年龄')\n",
    "        data = self._mode_fill_(data, '用户话费敏感度')\n",
    "        data = self._mode_fill_(data, '缴费用户最近一次缴费金额（元）')\n",
    "        data = self._mode_fill_(data, '用户话费敏感度')\n",
    "        data = self._feature_(data)\n",
    "        # data['年龄段'] = data['用户年龄'].apply(self._group_age_)\n",
    "        \n",
    "        data = self._log_feature_(data)\n",
    "        data=self._search_(data)\n",
    "        train, test = data[:50000], data[50000:]\n",
    "        test = test.drop(['信用分'], axis=1)\n",
    "\n",
    "        train_data_name = os.path.join(ETL_DATA_PATH, 'train_data.csv')\n",
    "        test_data_name = os.path.join(ETL_DATA_PATH, 'test_data.csv')\n",
    "        train.to_csv(train_data_name, index=False)\n",
    "        test.to_csv(test_data_name, index=False)\n",
    "        print('Gen train shape: {}, test shape: {}'.format(train.shape, test.shape))\n",
    "        print('features num: ', test.shape[1] - 1)\n",
    "\n",
    "\n",
    "if  __name__ == \"__main__\":\n",
    "    t0 = time.time()\n",
    "    processing = Processing()\n",
    "    processing.get_processing()\n",
    "    print(\"Feature engineering has finished!\")\n",
    "    print(\"Cost {} s.\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen train shape: (50000, 29), test shape: (50000, 28)\n",
      "features num:  27\n",
      "model: xgb_mae. fold:  0 training...\n",
      "0:\tlearn: 613.4190934\ttest: 613.4190934\ttest1: 613.0101538\tbest: 613.0101538 (0)\ttotal: 144ms\tremaining: 23m 57s\n",
      "500:\tlearn: 20.7709637\ttest: 20.7709637\ttest1: 21.1281294\tbest: 21.1281294 (500)\ttotal: 14.3s\tremaining: 4m 31s\n",
      "1000:\tlearn: 19.2295591\ttest: 19.2295591\ttest1: 19.6656169\tbest: 19.6656169 (1000)\ttotal: 27.5s\tremaining: 4m 7s\n",
      "1500:\tlearn: 18.8173254\ttest: 18.8173254\ttest1: 19.4209050\tbest: 19.4209050 (1500)\ttotal: 40.3s\tremaining: 3m 47s\n",
      "2000:\tlearn: 18.5246280\ttest: 18.5246280\ttest1: 19.3096321\tbest: 19.3096321 (2000)\ttotal: 53.6s\tremaining: 3m 34s\n",
      "2500:\tlearn: 18.2874143\ttest: 18.2874143\ttest1: 19.2375921\tbest: 19.2375921 (2500)\ttotal: 1m 6s\tremaining: 3m 18s\n",
      "3000:\tlearn: 18.0759012\ttest: 18.0759012\ttest1: 19.1910297\tbest: 19.1910105 (2998)\ttotal: 1m 18s\tremaining: 3m 4s\n",
      "3500:\tlearn: 17.8862067\ttest: 17.8862067\ttest1: 19.1562983\tbest: 19.1558057 (3497)\ttotal: 1m 31s\tremaining: 2m 49s\n",
      "4000:\tlearn: 17.7099652\ttest: 17.7099652\ttest1: 19.1334743\tbest: 19.1334743 (4000)\ttotal: 1m 44s\tremaining: 2m 36s\n",
      "4500:\tlearn: 17.5469455\ttest: 17.5469455\ttest1: 19.1178122\tbest: 19.1178122 (4498)\ttotal: 1m 57s\tremaining: 2m 23s\n",
      "5000:\tlearn: 17.3922787\ttest: 17.3922787\ttest1: 19.1047172\tbest: 19.1046557 (4980)\ttotal: 2m 9s\tremaining: 2m 9s\n",
      "5500:\tlearn: 17.2487481\ttest: 17.2487481\ttest1: 19.0953251\tbest: 19.0952158 (5488)\ttotal: 2m 22s\tremaining: 1m 56s\n",
      "6000:\tlearn: 17.1076921\ttest: 17.1076921\ttest1: 19.0867311\tbest: 19.0866426 (5991)\ttotal: 2m 34s\tremaining: 1m 43s\n",
      "6500:\tlearn: 16.9708557\ttest: 16.9708557\ttest1: 19.0797263\tbest: 19.0796437 (6490)\ttotal: 2m 48s\tremaining: 1m 30s\n",
      "7000:\tlearn: 16.8409871\ttest: 16.8409871\ttest1: 19.0753599\tbest: 19.0747747 (6942)\ttotal: 3m 1s\tremaining: 1m 17s\n",
      "7500:\tlearn: 16.7150127\ttest: 16.7150127\ttest1: 19.0762981\tbest: 19.0747113 (7269)\ttotal: 3m 14s\tremaining: 1m 4s\n",
      "8000:\tlearn: 16.5914521\ttest: 16.5914521\ttest1: 19.0720170\tbest: 19.0718211 (7866)\ttotal: 3m 26s\tremaining: 51.7s\n",
      "8500:\tlearn: 16.4744070\ttest: 16.4744070\ttest1: 19.0719744\tbest: 19.0708225 (8088)\ttotal: 3m 39s\tremaining: 38.7s\n",
      "9000:\tlearn: 16.3574071\ttest: 16.3574071\ttest1: 19.0702163\tbest: 19.0690538 (8743)\ttotal: 3m 51s\tremaining: 25.7s\n",
      "9500:\tlearn: 16.2421337\ttest: 16.2421337\ttest1: 19.0715513\tbest: 19.0690538 (8743)\ttotal: 4m 4s\tremaining: 12.8s\n",
      "9999:\tlearn: 16.1326011\ttest: 16.1326011\ttest1: 19.0696883\tbest: 19.0688694 (9737)\ttotal: 4m 17s\tremaining: 0us\n",
      "\n",
      "bestTest = 19.06886945\n",
      "bestIteration = 9737\n",
      "\n",
      "Shrink model to first 9738 iterations.\n",
      "model: xgb_mae. fold:  1 training...\n",
      "0:\tlearn: 613.3978037\ttest: 613.3978037\ttest1: 613.1002345\tbest: 613.1002345 (0)\ttotal: 24.5ms\tremaining: 4m 5s\n",
      "500:\tlearn: 20.8036280\ttest: 20.8036280\ttest1: 20.9286120\tbest: 20.9286120 (500)\ttotal: 12.5s\tremaining: 3m 56s\n",
      "1000:\tlearn: 19.2323455\ttest: 19.2323455\ttest1: 19.5760919\tbest: 19.5760919 (1000)\ttotal: 24.9s\tremaining: 3m 43s\n",
      "1500:\tlearn: 18.8154668\ttest: 18.8154668\ttest1: 19.3929132\tbest: 19.3929132 (1500)\ttotal: 37.3s\tremaining: 3m 31s\n",
      "2000:\tlearn: 18.5270272\ttest: 18.5270272\ttest1: 19.3056734\tbest: 19.3056707 (1999)\ttotal: 50.1s\tremaining: 3m 20s\n",
      "2500:\tlearn: 18.2823852\ttest: 18.2823852\ttest1: 19.2510344\tbest: 19.2507332 (2496)\ttotal: 1m 2s\tremaining: 3m 7s\n",
      "3000:\tlearn: 18.0752437\ttest: 18.0752437\ttest1: 19.2144595\tbest: 19.2143995 (2996)\ttotal: 1m 15s\tremaining: 2m 55s\n",
      "3500:\tlearn: 17.8847931\ttest: 17.8847931\ttest1: 19.1890116\tbest: 19.1890116 (3500)\ttotal: 1m 28s\tremaining: 2m 43s\n",
      "4000:\tlearn: 17.7124496\ttest: 17.7124496\ttest1: 19.1725032\tbest: 19.1724558 (3999)\ttotal: 1m 40s\tremaining: 2m 31s\n",
      "4500:\tlearn: 17.5486888\ttest: 17.5486888\ttest1: 19.1573095\tbest: 19.1568049 (4478)\ttotal: 1m 53s\tremaining: 2m 18s\n",
      "5000:\tlearn: 17.3947357\ttest: 17.3947357\ttest1: 19.1504771\tbest: 19.1499803 (4975)\ttotal: 2m 5s\tremaining: 2m 5s\n",
      "5500:\tlearn: 17.2470336\ttest: 17.2470336\ttest1: 19.1441429\tbest: 19.1436325 (5404)\ttotal: 2m 18s\tremaining: 1m 53s\n",
      "6000:\tlearn: 17.1063990\ttest: 17.1063990\ttest1: 19.1399841\tbest: 19.1399841 (6000)\ttotal: 2m 30s\tremaining: 1m 40s\n",
      "6500:\tlearn: 16.9696358\ttest: 16.9696358\ttest1: 19.1371571\tbest: 19.1371147 (6346)\ttotal: 2m 43s\tremaining: 1m 27s\n",
      "7000:\tlearn: 16.8374779\ttest: 16.8374779\ttest1: 19.1364884\tbest: 19.1362397 (6914)\ttotal: 2m 56s\tremaining: 1m 15s\n",
      "7500:\tlearn: 16.7082651\ttest: 16.7082651\ttest1: 19.1398828\tbest: 19.1362239 (7062)\ttotal: 3m 8s\tremaining: 1m 2s\n",
      "8000:\tlearn: 16.5849321\ttest: 16.5849321\ttest1: 19.1421910\tbest: 19.1362239 (7062)\ttotal: 3m 20s\tremaining: 50.2s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 19.13622391\n",
      "bestIteration = 7062\n",
      "\n",
      "Shrink model to first 7063 iterations.\n",
      "model: xgb_mae. fold:  2 training...\n",
      "0:\tlearn: 613.2109830\ttest: 613.2109830\ttest1: 613.8517998\tbest: 613.8517998 (0)\ttotal: 24.2ms\tremaining: 4m 2s\n",
      "500:\tlearn: 20.8714095\ttest: 20.8714095\ttest1: 20.8483129\tbest: 20.8483129 (500)\ttotal: 12.3s\tremaining: 3m 53s\n",
      "1000:\tlearn: 19.3279743\ttest: 19.3279743\ttest1: 19.3196533\tbest: 19.3196533 (1000)\ttotal: 24.9s\tremaining: 3m 43s\n",
      "1500:\tlearn: 18.9186495\ttest: 18.9186495\ttest1: 19.0667019\tbest: 19.0667019 (1500)\ttotal: 37.8s\tremaining: 3m 33s\n",
      "2000:\tlearn: 18.6263363\ttest: 18.6263363\ttest1: 18.9487010\tbest: 18.9487010 (2000)\ttotal: 50.2s\tremaining: 3m 20s\n",
      "2500:\tlearn: 18.3888825\ttest: 18.3888825\ttest1: 18.8826354\tbest: 18.8826354 (2500)\ttotal: 1m 2s\tremaining: 3m 7s\n",
      "3000:\tlearn: 18.1796747\ttest: 18.1796747\ttest1: 18.8419463\tbest: 18.8419463 (3000)\ttotal: 1m 14s\tremaining: 2m 54s\n",
      "3500:\tlearn: 17.9904744\ttest: 17.9904744\ttest1: 18.8154079\tbest: 18.8154034 (3499)\ttotal: 1m 27s\tremaining: 2m 41s\n",
      "4000:\tlearn: 17.8186874\ttest: 17.8186874\ttest1: 18.7926614\tbest: 18.7926614 (4000)\ttotal: 1m 39s\tremaining: 2m 29s\n",
      "4500:\tlearn: 17.6564465\ttest: 17.6564465\ttest1: 18.7723222\tbest: 18.7722679 (4494)\ttotal: 1m 52s\tremaining: 2m 17s\n",
      "5000:\tlearn: 17.5065055\ttest: 17.5065055\ttest1: 18.7621922\tbest: 18.7621922 (5000)\ttotal: 2m 5s\tremaining: 2m 5s\n",
      "5500:\tlearn: 17.3600575\ttest: 17.3600575\ttest1: 18.7501368\tbest: 18.7499464 (5496)\ttotal: 2m 17s\tremaining: 1m 52s\n",
      "6000:\tlearn: 17.2194099\ttest: 17.2194099\ttest1: 18.7456278\tbest: 18.7454819 (5968)\ttotal: 2m 29s\tremaining: 1m 39s\n",
      "6500:\tlearn: 17.0821924\ttest: 17.0821924\ttest1: 18.7414761\tbest: 18.7407615 (6436)\ttotal: 2m 42s\tremaining: 1m 27s\n",
      "7000:\tlearn: 16.9523620\ttest: 16.9523620\ttest1: 18.7356575\tbest: 18.7354017 (6976)\ttotal: 2m 55s\tremaining: 1m 15s\n",
      "7500:\tlearn: 16.8245252\ttest: 16.8245252\ttest1: 18.7339921\tbest: 18.7335836 (7365)\ttotal: 3m 8s\tremaining: 1m 2s\n",
      "8000:\tlearn: 16.7002319\ttest: 16.7002319\ttest1: 18.7336203\tbest: 18.7320469 (7617)\ttotal: 3m 20s\tremaining: 50.1s\n",
      "8500:\tlearn: 16.5805560\ttest: 16.5805560\ttest1: 18.7332033\tbest: 18.7320469 (7617)\ttotal: 3m 33s\tremaining: 37.6s\n",
      "9000:\tlearn: 16.4628566\ttest: 16.4628566\ttest1: 18.7321505\tbest: 18.7296593 (8906)\ttotal: 3m 45s\tremaining: 25.1s\n",
      "9500:\tlearn: 16.3485898\ttest: 16.3485898\ttest1: 18.7346669\tbest: 18.7296593 (8906)\ttotal: 3m 58s\tremaining: 12.6s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 18.72965925\n",
      "bestIteration = 8906\n",
      "\n",
      "Shrink model to first 8907 iterations.\n",
      "model: xgb_mae. fold:  3 training...\n",
      "0:\tlearn: 613.2097371\ttest: 613.2097371\ttest1: 613.8543975\tbest: 613.8543975 (0)\ttotal: 41.6ms\tremaining: 6m 56s\n",
      "500:\tlearn: 20.7965604\ttest: 20.7965604\ttest1: 20.9264159\tbest: 20.9264159 (500)\ttotal: 12.3s\tremaining: 3m 54s\n",
      "1000:\tlearn: 19.2376496\ttest: 19.2376496\ttest1: 19.6017910\tbest: 19.6017910 (1000)\ttotal: 25s\tremaining: 3m 45s\n",
      "1500:\tlearn: 18.8236155\ttest: 18.8236155\ttest1: 19.4051552\tbest: 19.4051552 (1500)\ttotal: 37.6s\tremaining: 3m 32s\n",
      "2000:\tlearn: 18.5320059\ttest: 18.5320059\ttest1: 19.3228535\tbest: 19.3228535 (2000)\ttotal: 50s\tremaining: 3m 19s\n",
      "2500:\tlearn: 18.2894442\ttest: 18.2894442\ttest1: 19.2728883\tbest: 19.2728883 (2500)\ttotal: 1m 2s\tremaining: 3m 7s\n",
      "3000:\tlearn: 18.0803467\ttest: 18.0803467\ttest1: 19.2420552\tbest: 19.2420205 (2999)\ttotal: 1m 14s\tremaining: 2m 54s\n",
      "3500:\tlearn: 17.8916486\ttest: 17.8916486\ttest1: 19.2202173\tbest: 19.2197867 (3488)\ttotal: 1m 27s\tremaining: 2m 42s\n",
      "4000:\tlearn: 17.7180858\ttest: 17.7180858\ttest1: 19.2085520\tbest: 19.2085520 (4000)\ttotal: 1m 40s\tremaining: 2m 30s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500:\tlearn: 17.5512386\ttest: 17.5512386\ttest1: 19.1953729\tbest: 19.1951348 (4493)\ttotal: 1m 52s\tremaining: 2m 17s\n",
      "5000:\tlearn: 17.3953362\ttest: 17.3953362\ttest1: 19.1946454\tbest: 19.1935275 (4925)\ttotal: 2m 5s\tremaining: 2m 5s\n",
      "5500:\tlearn: 17.2468680\ttest: 17.2468680\ttest1: 19.1946004\tbest: 19.1935275 (4925)\ttotal: 2m 18s\tremaining: 1m 53s\n",
      "6000:\tlearn: 17.1041578\ttest: 17.1041578\ttest1: 19.1946791\tbest: 19.1935200 (5880)\ttotal: 2m 31s\tremaining: 1m 41s\n",
      "6500:\tlearn: 16.9691565\ttest: 16.9691565\ttest1: 19.1942159\tbest: 19.1935200 (5880)\ttotal: 2m 44s\tremaining: 1m 28s\n",
      "7000:\tlearn: 16.8386903\ttest: 16.8386903\ttest1: 19.1945781\tbest: 19.1932342 (6606)\ttotal: 2m 57s\tremaining: 1m 16s\n",
      "7500:\tlearn: 16.7133230\ttest: 16.7133230\ttest1: 19.1982726\tbest: 19.1932342 (6606)\ttotal: 3m 10s\tremaining: 1m 3s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 19.19323418\n",
      "bestIteration = 6606\n",
      "\n",
      "Shrink model to first 6607 iterations.\n",
      "model: xgb_mae. fold:  4 training...\n",
      "0:\tlearn: 613.4523342\ttest: 613.4523342\ttest1: 612.8750437\tbest: 612.8750437 (0)\ttotal: 25.9ms\tremaining: 4m 18s\n",
      "500:\tlearn: 20.7676787\ttest: 20.7676787\ttest1: 21.0138097\tbest: 21.0138097 (500)\ttotal: 12.4s\tremaining: 3m 55s\n",
      "1000:\tlearn: 19.1974198\ttest: 19.1974198\ttest1: 19.7111686\tbest: 19.7111686 (1000)\ttotal: 25.7s\tremaining: 3m 50s\n",
      "1500:\tlearn: 18.7799196\ttest: 18.7799196\ttest1: 19.5072334\tbest: 19.5072334 (1500)\ttotal: 39.1s\tremaining: 3m 41s\n",
      "2000:\tlearn: 18.4875284\ttest: 18.4875284\ttest1: 19.4164030\tbest: 19.4164030 (2000)\ttotal: 52.3s\tremaining: 3m 29s\n",
      "2500:\tlearn: 18.2480716\ttest: 18.2480716\ttest1: 19.3714443\tbest: 19.3714443 (2500)\ttotal: 1m 4s\tremaining: 3m 14s\n",
      "3000:\tlearn: 18.0442985\ttest: 18.0442985\ttest1: 19.3399888\tbest: 19.3399888 (3000)\ttotal: 1m 17s\tremaining: 3m\n",
      "3500:\tlearn: 17.8565814\ttest: 17.8565814\ttest1: 19.3228368\tbest: 19.3227220 (3486)\ttotal: 1m 30s\tremaining: 2m 47s\n",
      "4000:\tlearn: 17.6813489\ttest: 17.6813489\ttest1: 19.3088209\tbest: 19.3085508 (3993)\ttotal: 1m 43s\tremaining: 2m 34s\n",
      "4500:\tlearn: 17.5147141\ttest: 17.5147141\ttest1: 19.3002627\tbest: 19.3001550 (4466)\ttotal: 1m 55s\tremaining: 2m 21s\n",
      "5000:\tlearn: 17.3611304\ttest: 17.3611304\ttest1: 19.2948695\tbest: 19.2941810 (4954)\ttotal: 2m 8s\tremaining: 2m 8s\n",
      "5500:\tlearn: 17.2166467\ttest: 17.2166467\ttest1: 19.2916161\tbest: 19.2910115 (5343)\ttotal: 2m 20s\tremaining: 1m 55s\n",
      "6000:\tlearn: 17.0732782\ttest: 17.0732782\ttest1: 19.2896170\tbest: 19.2884658 (5863)\ttotal: 2m 33s\tremaining: 1m 42s\n",
      "6500:\tlearn: 16.9372939\ttest: 16.9372939\ttest1: 19.2937472\tbest: 19.2884658 (5863)\ttotal: 2m 46s\tremaining: 1m 29s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 19.28846577\n",
      "bestIteration = 5863\n",
      "\n",
      "Shrink model to first 5864 iterations.\n",
      "xgb_mae cv score for valid is:  0.06386149836540181\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = get_feature(name=\"train_data\")\n",
    "test_data = get_feature(name=\"test_data\")\n",
    "\n",
    "print('Gen train shape: {}, test shape: {}'.format(train_data.shape, test_data.shape))\n",
    "print('features num: ', test_data.shape[1] - 1)\n",
    "# 'random_seed': 4590,\n",
    "    # lgb_mae参数\n",
    "ctb_params = {\n",
    "        'n_estimators': 10000,\n",
    "        'learning_rate': 0.01,\n",
    "        'random_seed': 5570,\n",
    "        'reg_lambda': 5,\n",
    "        'subsample': 0.7,\n",
    "        'bootstrap_type': 'Bernoulli',\n",
    "        'boosting_type': 'Plain',\n",
    "        'one_hot_max_size': 10,\n",
    "        'rsm': 0.5,\n",
    "        'leaf_estimation_iterations': 5,\n",
    "        'use_best_model': True,\n",
    "        'max_depth': 6,\n",
    "        'verbose': -1,\n",
    "        'thread_count': 4\n",
    "    }\n",
    "cat_mae_pred = cat_model(train_data, test_data, ctb_params)\n",
    "    #lgb_mae_pred = lgb_mae_model(train_data, test_data, params_mae_lgb)\n",
    "    #lgb_mse_pred = lgb_mse_model(train_data, test_data, params_mse_lgb)\n",
    "\n",
    "bagging_pred = cat_mae_pred\n",
    "\n",
    "test_data_sub = test_data[['用户编码']]\n",
    "    #test_data_sub['target']=tt[\"信用分\"]\n",
    "test_data_sub['score'] = bagging_pred\n",
    "test_data_sub.columns = ['id', 'score']\n",
    "test_data_sub['score'] = test_data_sub['score']\n",
    "test_data_sub['score'] = test_data_sub['score']#.apply(lambda x: int(np.round(x)))\n",
    "test_data_sub[['id', 'score']].to_csv('3.13_cat5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sub['score'] = test_data_sub['score'].apply(lambda x: int(np.round(x)))\n",
    "test_data_sub[['id', 'score']].to_csv('3.17_cat5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50000.000000\n",
       "mean       617.883780\n",
       "std         38.283659\n",
       "min        463.000000\n",
       "25%        596.000000\n",
       "50%        627.000000\n",
       "75%        646.000000\n",
       "max        700.000000\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_sub['score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  2.81 Mb (75.4% reduction)\n",
      "Mem. usage decreased to  2.81 Mb (74.6% reduction)\n",
      "Gen train shape: (50000, 32), test shape: (50000, 31)\n",
      "features num:  30\n",
      "Feature engineering has finished!\n",
      "Cost 2.5257692337036133 s.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 特征工程\n",
    "# author = 'huangth'\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "BASE_PATH = 'D:\\\\Azy\\\\'\n",
    "RAW_DATA_PATH = os.path.join(BASE_PATH, \"RawData\")\n",
    "ETL_DATA_PATH = os.path.join(BASE_PATH, \"EtlData\")\n",
    "def kuaidi(item):\n",
    "    if item==0:\n",
    "        return 0\n",
    "    if item>0 and item<=10:\n",
    "        return 1\n",
    "    if item>10 and item<=100:\n",
    "        return 2\n",
    "    if item>100:\n",
    "        return 3\n",
    "\n",
    "\n",
    "class Processing(object):\n",
    "    # 读取数据\n",
    "    @staticmethod\n",
    "    def _get_data_(name):\n",
    "        data_name = os.path.join(RAW_DATA_PATH, '{}.csv'.format(name))\n",
    "        df = pd.read_csv(data_name)\n",
    "        return df\n",
    "\n",
    "    # 改变变量类型节省内存空间\n",
    "    @staticmethod\n",
    "    def _reduce_mem_usage_(df, verbose=True):\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        start_mem = df.memory_usage().sum() / 1024**2\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)\n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "        return df\n",
    "\n",
    "    # 众数填充异常值\n",
    "    @staticmethod\n",
    "    def _mode_fill_(df, col):\n",
    "        df.loc[df[col] == 0, col] = df[col].mode()\n",
    "        return df\n",
    "\n",
    "    # 疯狂找特征呀呀呀\n",
    "    @staticmethod\n",
    "    def _feature_(df):\n",
    "        return df\n",
    "    @staticmethod\n",
    "    def get_recharge_way(item):\n",
    "        # 是否能被10整除\n",
    "        if item == 0:\n",
    "            return -1\n",
    "        if item % 10 == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # 长尾数据处理\n",
    "    @staticmethod\n",
    "    def _log_feature_(df):\n",
    "        return df\n",
    "    @staticmethod\n",
    "    def _gethob_(df):\n",
    "        #log_features = ['当月网购类应用使用次数', '当月金融理财类应用使用总次数', '当月视频播放类应用使用次数']\n",
    "        return df\n",
    "\n",
    "    def get_processing(self):\n",
    "        train_df = self._get_data_('train_dataset')\n",
    "        test_df = self._get_data_('test_dataset') \n",
    "        train_df = self._reduce_mem_usage_(train_df)\n",
    "        test_df = self._reduce_mem_usage_(test_df)\n",
    "        test_df['信用分'] = -1\n",
    "        data = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "        for col in ['当月网购类应用使用次数', '当月金融理财类应用使用总次数', '当月视频播放类应用使用次数',\n",
    "                    \"当月旅游资讯类应用使用次数\",'当月飞机类应用使用次数','当月火车类应用使用次数',\"当月物流快递类应用使用次数\",\n",
    "                    '近三个月月均商场出现次数']:\n",
    "            #large_high = np.percentile(data[col].values, 99.98)\n",
    "            high = np.percentile(data[col].values, 99.97)\n",
    "            mean = np.mean(data[col].values)\n",
    "            #data.loc[data[col] > large_high, col] = mean\n",
    "            data.loc[data[col] > high, col] = high\n",
    "        #data[\"用户年龄\"][data[\"用户年龄\"]==0]=data[\"用户年龄\"].mean()\n",
    "        data[\"用户近6个月平均消费值（元）\"][data[\"用户近6个月平均消费值（元）\"]<=1]=data[\"用户近6个月平均消费值（元）\"].mean()\n",
    "        data[\"用户账单当月总费用（元）\"][data[\"用户账单当月总费用（元）\"]<=1]=data[\"用户账单当月总费用（元）\"].mean()\n",
    "        data[\"消费稳定性2\"]=(data[\"用户账单当月总费用（元）\"]-data[\"用户近6个月平均消费值（元）\"]+1)/(data[\"用户近6个月平均消费值（元）\"]+1)\n",
    "        #data[\"还有几个\"]=data[\"用户账单当月总费用（元）\"]+data[\"用户当月账户余额（元）\"]\n",
    "        #data[\"交通\"]=data['当月飞机类应用使用次数']+data['当月飞机类应用使用次数']\n",
    "        #data=data.drop({'当月飞机类应用使用次数','当月火车类应用使用次数'},axis=1)\n",
    "            #data[col]=np.log(data[col]+1)\n",
    "            #df.loc[df[col] < low, col] = low\n",
    "        #for col in [\"用户账单当月总费用（元）\",\"用户近6个月平均消费值（元）\",\"用户当月账户余额（元）\",\"缴费用户最近一次缴费金额（元）\"]:\n",
    "        #    high = np.percentile(data[col].values, 99.999)\n",
    "        #    mean = np.mean(data[col].values)\n",
    "            #data.loc[data[col] > large_high, col] = mean\n",
    "        #    data.loc[data[col] > high, col] =mean   \n",
    "        \n",
    "        #data[\"上月余额\"]=(-data[\"用户账单当月总费用（元）\"]+data[\"用户当月账户余额（元）\"]+data[\"缴费用户最近一次缴费金额（元）\"]).astype(\"int64\")\n",
    "        #data[\"余额稳定性\"]=data[\"上月余额\"]/(data[\"用户当月账户余额（元）\"]+1)\n",
    "        #data[\"消费稳定性\"]=data[\"用户账单当月总费用（元）\"]/(1+data[\"用户近6个月平均消费值（元）\"])       \n",
    "        #data=data.drop({'当月是否逛过福州仓山万达','当月是否到过福州山姆会员店'},axis=1)   \n",
    "        #data=data.drop({\"用户最近一次缴费距今时长（月）\"},axis=1)\n",
    "        #data=data.drop({'是否经常逛商场的人'},axis=1)\n",
    "#    df=df.drop({'是否大学生客户'},axis=1)\n",
    "        #data=data.drop({'是否黑名单客户'},axis=1)\n",
    "        #data=data.drop(\"上月余额\",axis=1)\n",
    "        del train_df, test_df\n",
    "        gc.collect()\n",
    "        \n",
    "\n",
    "        data = self._log_feature_(data)\n",
    "\n",
    "        train, test = data[:50000], data[50000:]\n",
    "        test = test.drop(['信用分'], axis=1)\n",
    "\n",
    "        train_data_name = os.path.join(ETL_DATA_PATH, 'train_data.csv')\n",
    "        test_data_name = os.path.join(ETL_DATA_PATH, 'test_data.csv')\n",
    "        train.to_csv(train_data_name, index=False)\n",
    "        test.to_csv(test_data_name, index=False)\n",
    "        print('Gen train shape: {}, test shape: {}'.format(train.shape, test.shape))\n",
    "        print('features num: ', test.shape[1] - 1)\n",
    "\n",
    "\n",
    "if  __name__ == \"__main__\":\n",
    "    t0 = time.time()\n",
    "    processing = Processing()\n",
    "    processing.get_processing()\n",
    "    print(\"Feature engineering has finished!\")\n",
    "    print(\"Cost {} s.\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
